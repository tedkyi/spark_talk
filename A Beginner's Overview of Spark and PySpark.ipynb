{"cells":[{"cell_type":"markdown","source":["### Initialization"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.225.196:40490\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":2},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### Connection\nDatabricks notebooks are already connected to a cluster"],"metadata":{}},{"cell_type":"code","source":["# We already have a session called \"spark\"\nprint(type(spark))\n\n# We also have a spark context called \"sc\"\nprint(type(sc))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.sql.session.SparkSession&#39;&gt;\n&lt;class &#39;__main__.RemoteContext&#39;&gt;\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Pandas versus Spark DataFrames\nPandas DataFrames reside on the driver\nSpark DataFrames are distributed on partitions across the cluster"],"metadata":{}},{"cell_type":"code","source":["sparkDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/databricks-datasets/definitive-guide/data/retail-data/by-day/*.csv\")\n\nsparkDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = sparkDataFrame.schema\nprint(staticSchema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["sparkDataFrame.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nInvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["pandasDataFrame = sparkDataFrame.toPandas()\nprint(pandasDataFrame.shape)\npandasDataFrame.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>580538</td>\n      <td>23084</td>\n      <td>RABBIT NIGHT LIGHT</td>\n      <td>48</td>\n      <td>2011-12-05 08:38:00</td>\n      <td>1.79</td>\n      <td>14075.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>580538</td>\n      <td>23077</td>\n      <td>DOUGHNUT LIP GLOSS</td>\n      <td>20</td>\n      <td>2011-12-05 08:38:00</td>\n      <td>1.25</td>\n      <td>14075.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>580538</td>\n      <td>22906</td>\n      <td>12 MESSAGE CARDS WITH ENVELOPES</td>\n      <td>24</td>\n      <td>2011-12-05 08:38:00</td>\n      <td>1.65</td>\n      <td>14075.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>580538</td>\n      <td>21914</td>\n      <td>BLUE HARMONICA IN BOX</td>\n      <td>24</td>\n      <td>2011-12-05 08:38:00</td>\n      <td>1.25</td>\n      <td>14075.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>580538</td>\n      <td>22467</td>\n      <td>GUMBALL COAT RACK</td>\n      <td>6</td>\n      <td>2011-12-05 08:38:00</td>\n      <td>2.55</td>\n      <td>14075.0</td>\n      <td>United Kingdom</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["type(sparkDataFrame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: pyspark.sql.dataframe.DataFrame</div>"]}}],"execution_count":10},{"cell_type":"code","source":["sparkDataFrame.createOrReplaceTempView(\"purchases\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["type(pandasDataFrame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: pandas.core.frame.DataFrame</div>"]}}],"execution_count":12},{"cell_type":"code","source":["pandasDataFrame.createOrReplaceTempView(\"purchases\")"],"metadata":{"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2833190448324280&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>pandasDataFrame<span class=\"ansi-blue-fg\">.</span>createOrReplaceTempView<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;purchases&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/core/generic.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   5065</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_info_axis<span class=\"ansi-blue-fg\">.</span>_can_hold_identifiers_and_holds_name<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   5066</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">[</span>name<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">-&gt; 5067</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> object<span class=\"ansi-blue-fg\">.</span>__getattribute__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   5068</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   5069</span>     <span class=\"ansi-green-fg\">def</span> __setattr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">,</span> value<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;createOrReplaceTempView&#39;</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### Optimizing Execution Plans\nThe Catalyst optimizer takes advantage of lazy execution to optimize execution plans"],"metadata":{}},{"cell_type":"code","source":["# In the following cells, we will perform a series of transformations\n# The full set of steps is shown here for clarity\n# Note that the function withColumn() adds a new column\nfrom pyspark.sql.functions import lit, expr\n\ndf0 = spark.range(0, 40, 2).toDF(\"c0\")\ndf1 = df0.withColumn(\"c1\", lit(20))\ndf2 = df1.withColumn(\"c2\", expr(\"c1 - c0\"))\ndf3 = df2.withColumn(\"c3\", expr(\"c2 / 2\"))\ndf4 = df3.where(\"c3 % 2 = 0\")\ndf5 = df4.sort(\"c3\")\ndf6 = df5.withColumn(\"c4\", expr(\"c3 + 3\")).withColumn(\"c5\", expr(\"c4 * c4\")).withColumn(\"c6\", expr(\"c4 + c5\"))\ndf7 = df6.sort(\"c6\")\ndf7.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+----+----+-----+-----+\n c0| c1| c2|  c3|  c4|   c5|   c6|\n+---+---+---+----+----+-----+-----+\n 28| 20| -8|-4.0|-1.0|  1.0|  0.0|\n 24| 20| -4|-2.0| 1.0|  1.0|  2.0|\n 32| 20|-12|-6.0|-3.0|  9.0|  6.0|\n 20| 20|  0| 0.0| 3.0|  9.0| 12.0|\n 36| 20|-16|-8.0|-5.0| 25.0| 20.0|\n 16| 20|  4| 2.0| 5.0| 25.0| 30.0|\n 12| 20|  8| 4.0| 7.0| 49.0| 56.0|\n  8| 20| 12| 6.0| 9.0| 81.0| 90.0|\n  4| 20| 16| 8.0|11.0|121.0|132.0|\n  0| 20| 20|10.0|13.0|169.0|182.0|\n+---+---+---+----+----+-----+-----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# The code for df3 creates a range, then adds columns\n# The code has 4 steps, but Spark plan does it in 3 steps\ndf0 = spark.range(0, 40, 2).toDF(\"c0\")\ndf1 = df0.withColumn(\"c1\", lit(20))\ndf2 = df1.withColumn(\"c2\", expr(\"c1 - c0\"))\ndf3 = df2.withColumn(\"c3\", expr(\"c2 / 2\"))\ndf3.explain()\n\n# The code for df4 adds a filter as a 5th step\n# The Spark plan does the filter right after creating the range\n# This is what predicate pushdown does\n# It eliminates the need to calculate column values for the rows that eventually are filtered out\ndf4 = df3.where(\"c3 % 2 = 0\")\ndf4.explain()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(1) Project [c0#144L, 20 AS c1#146, c2#149L, (cast(c2#149L as double) / 2.0) AS c3#153]\n+- *(1) Project [id#142L AS c0#144L, (20 - id#142L) AS c2#149L]\n   +- *(1) Range (0, 40, step=2, splits=8)\n== Physical Plan ==\n*(1) Project [c0#144L, 20 AS c1#146, c2#149L, (cast(c2#149L as double) / 2.0) AS c3#153]\n+- *(1) Project [id#142L AS c0#144L, (20 - id#142L) AS c2#149L]\n   +- *(1) Filter (((cast((20 - id#142L) as double) / 2.0) % 2.0) = 0.0)\n      +- *(1) Range (0, 40, step=2, splits=8)\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# The code for df6 does a sort and adds more columns\ndf5 = df4.sort(\"c3\")\ndf6 = df5.withColumn(\"c4\", expr(\"c3 + 3\")).withColumn(\"c5\", expr(\"c4 * c4\")).withColumn(\"c6\", expr(\"c4 + c5\"))\ndf6.explain()\n\n# The code for df7 adds another sort\n# The Spark plan only sorts the data once\ndf7 = df6.sort(\"c6\")\ndf7.explain()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Project [c0#144L, 20 AS c1#146, c2#149L, c3#153, c4#158, c5#164, (c4#158 + c5#164) AS c6#171]\n+- *(2) Project [c0#144L, c2#149L, c3#153, c4#158, (c4#158 * c4#158) AS c5#164]\n   +- *(2) Project [c0#144L, c2#149L, c3#153, (c3#153 + 3.0) AS c4#158]\n      +- *(2) Sort [c3#153 ASC NULLS FIRST], true, 0\n         +- Exchange rangepartitioning(c3#153 ASC NULLS FIRST, 4), [id=#211]\n            +- *(1) Project [c0#144L, c2#149L, (cast(c2#149L as double) / 2.0) AS c3#153]\n               +- *(1) Project [id#142L AS c0#144L, (20 - id#142L) AS c2#149L]\n                  +- *(1) Filter (((cast((20 - id#142L) as double) / 2.0) % 2.0) = 0.0)\n                     +- *(1) Range (0, 40, step=2, splits=8)\n== Physical Plan ==\nSort [c6#171 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(c6#171 ASC NULLS FIRST, 4), [id=#286]\n   +- *(1) Project [c0#144L, 20 AS c1#146, c2#149L, c3#153, c4#158, c5#164, (c4#158 + c5#164) AS c6#171]\n      +- *(1) Project [c0#144L, c2#149L, c3#153, c4#158, (c4#158 * c4#158) AS c5#164]\n         +- *(1) Project [c0#144L, c2#149L, c3#153, (c3#153 + 3.0) AS c4#158]\n            +- *(1) Project [c0#144L, c2#149L, (cast(c2#149L as double) / 2.0) AS c3#153]\n               +- *(1) Project [id#142L AS c0#144L, (20 - id#142L) AS c2#149L]\n                  +- *(1) Filter (((cast((20 - id#142L) as double) / 2.0) % 2.0) = 0.0)\n                     +- *(1) Range (0, 40, step=2, splits=8)\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Spark SQL\nSome computations are easier to express in SQL than with traditional Pandas-like code"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  SELECT CustomerId, count(Quantity) AS num, SUM(UnitPrice * Quantity) AS total_cost\n  FROM purchases\n  WHERE CustomerId IN (SELECT CustomerId FROM purchases WHERE CustomerID < 14000)\n  GROUP BY CustomerId\n  HAVING count(Quantity) > 500\n  \"\"\")\\\n  .collect()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: [Row(CustomerId=13089.0, num=1857, total_cost=57385.87999999997),\n Row(CustomerId=13230.0, num=612, total_cost=2763.409999999999),\n Row(CustomerId=13137.0, num=705, total_cost=3605.080000000001),\n Row(CustomerId=12867.0, num=551, total_cost=3986.2199999999993),\n Row(CustomerId=12471.0, num=531, total_cost=18740.920000000002),\n Row(CustomerId=12921.0, num=741, total_cost=16389.739999999998),\n Row(CustomerId=13969.0, num=633, total_cost=8986.690000000002),\n Row(CustomerId=12681.0, num=646, total_cost=13677.590000000004),\n Row(CustomerId=12748.0, num=4642, total_cost=29072.09999999999),\n Row(CustomerId=13081.0, num=1061, total_cost=27964.479999999996),\n Row(CustomerId=13263.0, num=1677, total_cost=7454.0700000000015),\n Row(CustomerId=13668.0, num=501, total_cost=6216.070000000001),\n Row(CustomerId=13694.0, num=585, total_cost=62653.1),\n Row(CustomerId=12415.0, num=778, total_cost=123725.45000000001),\n Row(CustomerId=12682.0, num=525, total_cost=12288.219999999998),\n Row(CustomerId=13408.0, num=501, total_cost=27487.410000000003),\n Row(CustomerId=13098.0, num=605, total_cost=28658.879999999994)]</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### Machine Learning\nThe following cells show a simple model being trained"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col\npreppedDataFrame = sparkDataFrame\\\n  .na.fill(0)\\\n  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n  .coalesce(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["trainDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate < '2011-07-01'\")\ntestDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate >= '2011-07-01'\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer()\\\n  .setInputCol(\"day_of_week\")\\\n  .setOutputCol(\"day_of_week_index\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder()\\\n  .setInputCol(\"day_of_week_index\")\\\n  .setOutputCol(\"day_of_week_encoded\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler()\\\n  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n  .setOutputCol(\"features\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\ntransformationPipeline = Pipeline()\\\n  .setStages([indexer, encoder, vectorAssembler])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["fittedPipeline = transformationPipeline.fit(trainDataFrame)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["transformedTraining = fittedPipeline.transform(trainDataFrame)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["transformedTraining.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\nInvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|day_of_week_index|day_of_week_encoded|            features|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\n   537226|    22811|SET OF 6 T-LIGHTS...|       6|2010-12-06 08:34:00|     2.95|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.95,...|\n   537226|    21713|CITRONELLA CANDLE...|       8|2010-12-06 08:34:00|      2.1|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.1,8...|\n   537226|    22927|GREEN GIANT GARDE...|       2|2010-12-06 08:34:00|     5.95|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[5.95,...|\n   537226|    20802|SMALL GLASS SUNDA...|       6|2010-12-06 08:34:00|     1.65|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.65,...|\n   537226|    22052|VINTAGE CARAVAN G...|      25|2010-12-06 08:34:00|     0.42|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.42,...|\n   537226|    22705|   WRAP GREEN PEARS |      25|2010-12-06 08:34:00|     0.42|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.42,...|\n   537226|    20781|GOLD EAR MUFF HEA...|       2|2010-12-06 08:34:00|     5.49|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[5.49,...|\n   537226|    22310|IVORY KNITTED MUG...|       6|2010-12-06 08:34:00|     1.65|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.65,...|\n   537226|    22389|PAPERWEIGHT SAVE ...|       6|2010-12-06 08:34:00|     2.55|   15987.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.55,...|\n   537227|    22941|CHRISTMAS LIGHTS ...|       2|2010-12-06 08:42:00|      8.5|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[8.5,2...|\n   537227|    22696| WICKER WREATH LARGE|       6|2010-12-06 08:42:00|     1.95|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.95,...|\n   537227|    22193|RED DINER WALL CLOCK|       2|2010-12-06 08:42:00|      8.5|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[8.5,2...|\n   537227|    21212|PACK OF 72 RETROS...|     120|2010-12-06 08:42:00|     0.42|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.42,...|\n   537227|    21977|PACK OF 60 PINK P...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.55,...|\n   537227|    84991|60 TEATIME FAIRY ...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.55,...|\n   537227|    21213|PACK OF 72 SKULL ...|      48|2010-12-06 08:42:00|     0.55|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.55,...|\n   537227|    21080|SET/20 RED RETROS...|      12|2010-12-06 08:42:00|     0.85|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.85,...|\n   537227|    22632|HAND WARMER RED R...|      48|2010-12-06 08:42:00|      2.1|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.1,4...|\n   537227|    22315|200 RED + WHITE B...|      12|2010-12-06 08:42:00|     1.25|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n   537227|    21232|STRAWBERRY CERAMI...|      12|2010-12-06 08:42:00|     1.25|   17677.0|United Kingdom|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.25,...|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-----------------+-------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nkmeans = KMeans()\\\n  .setK(20)\\\n  .setSeed(1)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["kmModel = kmeans.fit(transformedTraining)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["summary = kmModel.summary\nprint(\"Cluster Sizes:\")\nprint(summary.clusterSizes)\ncenters = kmModel.clusterCenters()\nprint(\"Cluster Centers:\")\nfor center in centers:\n  print(center)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Cluster Sizes:\n[234864, 1, 1, 1, 8, 4, 2, 9025, 4, 144, 18, 6, 1414, 4, 11, 199, 5, 46, 99, 47]\nCluster Centers:\n[3.95038286 5.4204348  0.19555147 0.19409105 0.1805811  0.17158866\n 0.14926511]\n[1.0400e+00 7.4215e+04 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n 0.0000e+00]\n[ 1.0400e+00 -7.4215e+04  0.0000e+00  1.0000e+00  0.0000e+00  0.0000e+00\n  0.0000e+00]\n[ 3.897e+04 -1.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n  1.000e+00]\n[ 5.43415e+03 -1.00000e+00  0.00000e+00  1.25000e-01  3.75000e-01\n  0.00000e+00  5.00000e-01]\n[ 7.5000e-03 -9.4045e+03  2.5000e-01  7.5000e-01  0.0000e+00  0.0000e+00\n  0.0000e+00]\n[ 1.6670865e+04 -1.0000000e+00  0.0000000e+00  0.0000000e+00\n  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n[ 1.51478227 62.18027701  0.21396122  0.21218837  0.13285319  0.21518006\n  0.15102493]\n[3.8500e-01 4.4435e+03 2.5000e-01 2.5000e-01 0.0000e+00 0.0000e+00\n 5.0000e-01]\n[1.01923611e+00 6.98694444e+02 2.77777778e-01 2.36111111e-01\n 1.31944444e-01 2.15277778e-01 1.04166667e-01]\n[ 1.90522556e+03 -1.11111111e-01  5.55555556e-02  1.66666667e-01\n  3.88888889e-01  5.55555556e-02  3.33333333e-01]\n[ 3.50000000e-01 -3.15500000e+03  0.00000000e+00  1.66666667e-01\n  3.33333333e-01  5.00000000e-01  0.00000000e+00]\n[1.35835926e+00 1.89192362e+02 2.43281471e-01 2.39038190e-01\n 1.23762376e-01 2.05091938e-01 1.49222065e-01]\n[ 1.3524695e+04 -5.0000000e-01  0.0000000e+00  1.0000000e+00\n  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n[6.54545455e-01 2.66545455e+03 2.72727273e-01 9.09090909e-02\n 1.81818182e-01 1.81818182e-01 1.81818182e-01]\n[1.31020101e+00 4.15376884e+02 3.06532663e-01 1.95979899e-01\n 1.80904523e-01 1.90954774e-01 1.10552764e-01]\n[ 7.385808e+03 -6.000000e-01  0.000000e+00  8.000000e-01  2.000000e-01\n  0.000000e+00  0.000000e+00]\n[ 4.19130435e-01 -1.05697826e+03  1.73913043e-01  2.39130435e-01\n  2.39130435e-01  2.17391304e-01  1.30434783e-01]\n[6.84855758e+02 9.09090909e-01 1.91919192e-01 2.32323232e-01\n 2.82828283e-01 1.01010101e-01 1.91919192e-01]\n[7.46382979e-01 1.38591489e+03 2.34042553e-01 1.91489362e-01\n 1.06382979e-01 1.91489362e-01 2.12765957e-01]\n</div>"]}}],"execution_count":32}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset","notebookId":2833190448324267},"nbformat":4,"nbformat_minor":0}
